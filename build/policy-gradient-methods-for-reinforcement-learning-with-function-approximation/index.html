<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Policy Gradient Methods for Reinforcement Learning with Function Approximation | Tim Whitaker</title><link rel="icon" href="/static/favicon.ico"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,500,700|Fira+Sans:400,500,700&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"><link rel="stylesheet" href="/static/styles/reset.css"><link rel="stylesheet" href="/static/styles/theme.css"><link rel="stylesheet" href="/static/styles/article.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script></head><body><header><div class="wrapper"><div class="masthead"><div class="logo"><a class="title" href="/">Tim Whitaker</a></div><nav><a href="/">  <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="home" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" class="svg-inline--fa fa-home fa-w-18 fa-3x"><path fill="currentColor" d="M280.37 148.26L96 300.11V464a16 16 0 0 0 16 16l112.06-.29a16 16 0 0 0 15.92-16V368a16 16 0 0 1 16-16h64a16 16 0 0 1 16 16v95.64a16 16 0 0 0 16 16.05L464 480a16 16 0 0 0 16-16V300L295.67 148.26a12.19 12.19 0 0 0-15.3 0zM571.6 251.47L488 182.56V44.05a12 12 0 0 0-12-12h-56a12 12 0 0 0-12 12v72.61L318.47 43a48 48 0 0 0-61 0L4.34 251.47a12 12 0 0 0-1.6 16.9l25.5 31A12 12 0 0 0 45.15 301l235.22-193.74a12.19 12.19 0 0 1 15.3 0L530.9 301a12 12 0 0 0 16.9-1.6l25.5-31a12 12 0 0 0-1.7-16.93z" class=""></path></svg>
</a><a href="/analysis" target="_blank">  <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="code" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" class="svg-inline--fa fa-code fa-w-20 fa-3x"><path fill="currentColor" d="M278.9 511.5l-61-17.7c-6.4-1.8-10-8.5-8.2-14.9L346.2 8.7c1.8-6.4 8.5-10 14.9-8.2l61 17.7c6.4 1.8 10 8.5 8.2 14.9L293.8 503.3c-1.9 6.4-8.5 10.1-14.9 8.2zm-114-112.2l43.5-46.4c4.6-4.9 4.3-12.7-.8-17.2L117 256l90.6-79.7c5.1-4.5 5.5-12.3.8-17.2l-43.5-46.4c-4.5-4.8-12.1-5.1-17-.5L3.8 247.2c-5.1 4.7-5.1 12.8 0 17.5l144.1 135.1c4.9 4.6 12.5 4.4 17-.5zm327.2.6l144.1-135.1c5.1-4.7 5.1-12.8 0-17.5L492.1 112.1c-4.8-4.5-12.4-4.3-17 .5L431.6 159c-4.6 4.9-4.3 12.7.8 17.2L523 256l-90.6 79.7c-5.1 4.5-5.5 12.3-.8 17.2l43.5 46.4c4.5 4.9 12.1 5.1 17 .6z" class=""></path></svg>
</a></nav></div></div></header><main><div class="wrapper"><div class="grid"><article><h1 id="policy-gradient-methods-for-reinforcement-learning-with-function-approximation-1">Policy Gradient Methods for Reinforcement Learning with Function Approximation</h1>
<p>Authors: Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour Source: <a href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf" class="uri">https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf</a></p>
<p>This paper introduces an approach to reinforcement learning that uses function approximators to represent a stochastic policy. The authors go on to prove that updating the parameters of the function approximator with respect to the expected reward is convergent to a local optimum.</p>
<p>The policy gradient approach updates the parameters <span class="math inline"><em>θ</em></span> with respect to the policy performance <span class="math inline"><em>ρ</em></span> by a step size <span class="math inline"><em>α</em></span>.</p>
<p><br /><span class="math display">$$
\Delta \theta \approx \alpha \frac{\partial \rho}{\partial \theta}
$$</span><br /></p>
<h2 id="policy-gradient-theorem">Policy Gradient Theorem</h2>
<p>With function approximation, there are two ways of formulating the agent's objective. The average-reward formulation, in which policies are ranked according to their long term expected reward per step, and the start-state formulation, in which the policies are ranked according to the expected long term reward of a given start state.</p>
<p>The performance of a policy and the value of a state-action pair with an average-reward formulation is:</p>
<p><br /><span class="math display">$$
\rho(\pi) = \lim_{n\to\infty} \frac{1}{n} E(r_1 + r_2 + ... + r_n)
$$</span><br /></p>
<p><br /><span class="math display">$$
Q^\pi(s,a) = \sum_{t=1}^{\infty} E(r_t - \rho(\pi))
$$</span><br /></p>
<p>The performance of a policy and the value of a state-action pair with a start-state formulation is:</p>
<p><br /><span class="math display">$$
\rho(\pi) = E \sum_{t=1}^{\infty} \gamma^{t-1} r_t
$$</span><br /></p>
<p><br /><span class="math display">$$
Q^\pi(s,a) = E \sum_{k=1}^{\infty} \gamma^{k-1} r_{t+k}
$$</span><br /></p>
<p>For both of these formulations, the policy gradient for any Markov Decision Process can be defined as</p>
<p><br /><span class="math display">$$
\frac{\partial \rho}{\partial \theta} = \sum_s d^{\pi}(s) \sum_a \frac{\partial \pi(s, a)}{\partial \theta} Q^{\pi}(s, a)
$$</span><br /></p>
<h2 id="policy-gradient-with-function-approximation">Policy Gradient with Function Approximation</h2>
<p>The state-action value function <span class="math inline"><em>Q</em><sup><em>π</em></sup></span> is not normally known and must be estimated. If a learned function approximator <span class="math inline"><em>f</em><sub><em>w</em></sub></span> is sufficiently good, we can use it in place of <span class="math inline"><em>Q</em><sup><em>π</em></sup></span> and still point roughly in the direction of the gradient.</p>
<p>When <span class="math inline"><em>f</em><sub><em>w</em></sub></span> converges to a local optimum, we get:</p>
<p><br /><span class="math display">$$
\sum_s d^{\pi}(s) \sum_a  \frac{\partial \pi(s, a)}{\partial \theta}[Q^{\pi}(s, a) - f_w(s,a)] = 0
$$</span><br /></p>
<p>which tells us that the error in <span class="math inline"><em>f</em><sub><em>w</em></sub>(<em>s</em>, <em>a</em>)</span> is orthogonal to the gradient of the policy parameterization. Because the expression is 0 at the local optimum, we can subtract it from the policy gradient to get:</p>
<p><br /><span class="math display">$$
\begin{aligned}
\frac{\partial \rho}{\partial \theta} &amp;= \sum_s d^{\pi}(s) \sum_a \frac{\partial \pi(s, a)}{\partial \theta} Q^{\pi}(s, a) - \sum_s d^{\pi}(s) \sum_a  \frac{\partial \pi(s, a)}{\partial \theta}[Q^{\pi}(s, a) - f_w(s,a)] \\\\
&amp;= \sum_s d^{\pi}(s)  \sum_a  \frac{\partial \pi(s, a)}{\partial \theta}[Q^{\pi}(s, a) - Q^{\pi}(s,a) + f_w(s,a)] \\\\
&amp;= \sum_s d^{\pi}(s)  \sum_a  \frac{\partial \pi(s, a)}{\partial \theta} f_w(s,a)
\end{aligned}
$$</span><br /></p>
<h2 id="policy-iteration-with-function-approximation">Policy Iteration with Function Approximation</h2>
<p>Given the previous theorems, we can prove that a form of policy iteration is convergent to a locally optimal policy.</p>
<p><br /><span class="math display">$$
\theta_{k+1} = \theta_k + \alpha_k \sum_s d^{\pi_k}(s) \sum_a \frac{\partial \pi_k(s,a)}{\partial \theta} f_{w_k}(s,a)
$$</span><br /></p>
<h1 id="related">Related</h1>
<ul>
<li><a href="file:///reinforcement-learning">Reinforcement Learning</a></li>
</ul>
</article><aside><h5 class="minion">Meta</h5><div class="meta"><img src="/static/images/programming.svg"><ul class="links"><li><a href="https://github.com/tjwhitaker">https://github.com/tjwhitaker</a></li><li><a href="#">mail@timwhitaker.ai</a></li></ul></div><h5 class="minion">Subscribe</h5><div class="newsletter"><h3>Want to stay updated?</h3><input type="email" placeholder="Email"><button>Sign Me Up</button></div></aside></div></div></main><footer><div class="wrapper"></div></footer><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body, {'delimiters': [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}]});"></script></body></html>