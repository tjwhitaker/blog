<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Home | Tim Whitaker</title><link rel="icon" href="/static/favicon.ico"><link rel="stylesheet" href="/static/styles/reset.css"><link rel="stylesheet" href="/static/styles/theme.css"><link rel="stylesheet" href="/static/styles/index.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,500,700|Fira+Sans:400,500,700&display=swap"></head><body><header><div class="wrapper"><div class="masthead"><div class="logo"><a class="title" href="/">Tim Whitaker</a></div><nav><a href="/">  <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="home" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" class="svg-inline--fa fa-home fa-w-18 fa-3x"><path fill="currentColor" d="M280.37 148.26L96 300.11V464a16 16 0 0 0 16 16l112.06-.29a16 16 0 0 0 15.92-16V368a16 16 0 0 1 16-16h64a16 16 0 0 1 16 16v95.64a16 16 0 0 0 16 16.05L464 480a16 16 0 0 0 16-16V300L295.67 148.26a12.19 12.19 0 0 0-15.3 0zM571.6 251.47L488 182.56V44.05a12 12 0 0 0-12-12h-56a12 12 0 0 0-12 12v72.61L318.47 43a48 48 0 0 0-61 0L4.34 251.47a12 12 0 0 0-1.6 16.9l25.5 31A12 12 0 0 0 45.15 301l235.22-193.74a12.19 12.19 0 0 1 15.3 0L530.9 301a12 12 0 0 0 16.9-1.6l25.5-31a12 12 0 0 0-1.7-16.93z" class=""></path></svg>
</a><a href="https://github.com/whitsworks" target="_blank">  <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="code" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" class="svg-inline--fa fa-code fa-w-20 fa-3x"><path fill="currentColor" d="M278.9 511.5l-61-17.7c-6.4-1.8-10-8.5-8.2-14.9L346.2 8.7c1.8-6.4 8.5-10 14.9-8.2l61 17.7c6.4 1.8 10 8.5 8.2 14.9L293.8 503.3c-1.9 6.4-8.5 10.1-14.9 8.2zm-114-112.2l43.5-46.4c4.6-4.9 4.3-12.7-.8-17.2L117 256l90.6-79.7c5.1-4.5 5.5-12.3.8-17.2l-43.5-46.4c-4.5-4.8-12.1-5.1-17-.5L3.8 247.2c-5.1 4.7-5.1 12.8 0 17.5l144.1 135.1c4.9 4.6 12.5 4.4 17-.5zm327.2.6l144.1-135.1c5.1-4.7 5.1-12.8 0-17.5L492.1 112.1c-4.8-4.5-12.4-4.3-17 .5L431.6 159c-4.6 4.9-4.3 12.7.8 17.2L523 256l-90.6 79.7c-5.1 4.5-5.5 12.3-.8 17.2l43.5 46.4c4.5 4.9 12.1 5.1 17 .6z" class=""></path></svg>
</a><a href="#">  <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="envelope" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="svg-inline--fa fa-envelope fa-w-16 fa-3x"><path fill="currentColor" d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z" class=""></path></svg>
</a></nav></div></div></header><div class="wrapper"><div class="grid"><main><div class="posts"><div class="post"><h2 class="title">2020-01-27</h2><p>Met with Darrell today about research. Going to partition some of this work so that I caan do my thesis on a study of generalization. Short term plans are to reproduce all of the generalization experiments for the nk ensemble network.</p>
</div><div class="post"><h2 class="title">2020-01-26</h2><p>Keeping the log streak alive. I can see why twitter is so appealing. Short updates are easy to do and skip that perfectionist procrastination where you can spend weeks writing and rewriting. No research today. Putting all my energy into caring for Alyssa.</p>
</div><div class="post"><h2 class="title">2020-01-25</h2><p>Why isn&#39;t there more research being done on generalization? Is it because it&#39;s hard? Broad? Vague? Is tweaking another vision model the easiest way to build applications and get publications? Artefeceal general intelligence is the true grand challenge in this field. I find it funny how few people are actually working on this problem.</p>
</div><div class="post"><h2 class="title">2020-01-24</h2><p>Solving systems of linear equations via gaussian elimination. Learning about row reduced echelon forms, elementary matrices and row and column wise elementary operations.</p>
</div><div class="post"><h2 class="title">2020-01-23</h2><p>Practicing linear algebra problems with Julia. Also thinking about how I&#39;m going to structure my thesis. How do I organize a study of generalizations of neural networks across a number of different architectures and tasks? How can we apply neuron group selection to a number of different architectures? How can we train networks without gradient descent?</p>
</div><div class="post"><h2 class="title">2020-01-22</h2><p>Worked on a set of linear algebra problems today. Practicing basic operations on matrices and vectors.</p>
</div><div class="post"><h2 class="title">2020-01-21</h2><p>Ran some experiments today. Training time was ~5 hours for the 4 experiments. Ran each of the 4 experiments for 100 runs. Tested n &#61; &#91;50, 100&#93; and k &#61; &#91;2, 5&#93;. All experiments used a reservoir size of 100. Results for the final runs are:</p>
<pre><code>&#40;n&#61;50, k&#61;2&#41;: 1.3880128966494905

&#40;n&#61;50, k&#61;5&#41;: 1.88251060467359

&#40;n&#61;100, k&#61;2&#41;: 3.306071506585949

&#40;n&#61;100, k&#61;5&#41;: 3.57770392965371</code></pre>
</div><div class="post"><h2 class="title">2020-01-20</h2><p>Finished formatting the entire repo. Set up a clang-format config so Dave and I should both be able to use the same formatting styles between our different environments.</p>
</div><div class="post"><h2 class="title">2020-01-19</h2><p>Continued work on formatting. Got through the entire echo state class today. Also started profiling performance of the first algorithm with valgrind. Seeing a lot of computation spent on matrix multiplication and related functions. If we want to speed our code up and enable gpu acceleration, I think these functions are a good place to start.</p>
</div><div class="post"><h2 class="title">2020-01-18</h2><p>Cleaned up code in nnet2nk.cpp for the nkrc project. Added line breaks, spaces, and indentation to make the code more readable. I started refactoring as well but ended up reverting those changes. I think it&#39;s best to fully understand the current system before I go changing it. I added comments to every functional block of code in nnet2nk.cpp.</p>
</div><div class="post"><h2 class="title">2020-01-09</h2><p>Rereading some papersto get reaquainted with the project. Read next generation genetic algorithms and nk ensemble networks. I&#39;m really excited about applying partition crossover to reservoirs. I think right now, the nk project doesn&#39;t actually use partition crossover, but I think it could be an interesting addition in the future. Not only can we select the best grouping of neurons for any given task, we can also evolve the configurations of the neurons as well as perform this neuron group selection.</p>
</div><div class="post"><h2 class="title">2020-01-08</h2><p>Need to spend a day resting after all that driving. It&#39;s good to be back home.</p>
</div><div class="post"><h2 class="title">2020-01-07</h2><p>In New Mexico. No research bieng done on this leg. Just driving.</p>
</div><div class="post"><h2 class="title">2020-01-06</h2><p>Driving back to Colorado. Will have almost 5000 miles logged for this trip.</p>
</div><div class="post"><h2 class="title">2020-01-05</h2><p>Why is generalization so hard? I think the future of machine learning will need to include large ensembles of different models. How can we teach machines to learn if their networks are as specialized as they are now?</p>
</div><div class="post"><h2 class="title">2020-01-04</h2><p>More reading about echo state networks. From what I can tell, ESNs outperform small artificial neural networks of comparable sizes. They perform really well for lower dimensional tasks. These include time series and simple reinforcement learning. They have not been shown to outperform really deep networks like state of the art CNNs on high dimensional tasks like image processing.</p>
</div><div class="post"><h2 class="title">2020-01-03</h2><p>Learning about reservoir computing architectures. The two main flavors are echo state networks and liquid state machines. They both include reservoirs of sparsely connected recurrent neurons. The difference between ESNs and LSMs are in the activation of the neurons. ESNs are like any other standard ANN neuron. LSMs use spiking neurons, they only fire when their aggregate inputs sum up to a value greater than rome threshold.</p>
</div><div class="post"><h2 class="title">2020-01-02</h2><p>Excited to start my research soon&#33; Read a couple papers that Dr. Whitley sent me. Next generation genetic algorithms and nk ensemble networks. These papers will form the foundation underlying much of my research.</p>
</div><div class="post"><h2 class="title">2020-01-01</h2><p>Happy new year and welcome to my new website&#33; Hope all of you had a nice holiday season. Alyssa and I got engaged over ours&#33; It was full of fun and family and celebration. We are so happy and can&#39;t wait to see what&#39;s in store for the next decade&#33;</p>
</div></div></main><aside><h5 class="minion">Meta</h5><div class="meta"><img src="/static/images/programming.svg"><p>Hello everyone. This is a collection of short and simple logs about work and life.</p></div><h5 class="minion">Subscribe</h5><div class="newsletter"><h3>Want to stay updated?</h3><input type="email" placeholder="Email"><button>Sign Me Up</button></div></aside></div></div><footer><div class="wrapper"></div></footer></body></html>