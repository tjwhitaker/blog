#+TITLE: Trust Region Policy Optimization
#+DESCRIPTION:
#+STARTUP: latexpreview

* Trust Region Policy Optimization

Authors: John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel
Source: https://arxiv.org/pdf/1502.05477.pdf

When using policy gradients, small changes in the parameter space can sometimes have very large differences in performance. This makes it dangerous to use large step sizes as a single bad step can collapse policy performance. TRPO introduces a method for updating the policy while satisfying a constraint that measures how similar new and old policies are allowed to be.

** Key Equations

Maximize the expectation of the surrogate advantage.

$$
\begin{aligned}
maximize \ \ &\mathbb{E} \left[ \frac{\pi_{\theta}(a | s)}{\pi_{\theta_{old}}(a | s)} A_{\theta_{old}}(s,a) \right] \\\\
subject\ to \ \ &\mathbb{E}\ [ D_{KL}(\pi_{\theta_{old}}(\cdot | s)\ ||\  \pi_{\theta}(\cdot | s))] \ \leq \ \delta \\\\
\end{aligned}
$$
