#+TITLE: Policy Gradient Methods for Reinforcement Learning with Function Approximation
#+DESCRIPTION: Timeless classic of reinforcment learning theory.
#+STARTUP: latexpreview

* Policy Gradient Methods for Reinforcement Learning with Function Approximation

Authors: Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour
Source: https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf

This paper introduces an approach to reinforcement learning that uses function approximators to represent a stochastic policy. The authors go on to prove that updating the parameters of the function approximator with respect to the expected reward is convergent to a local optimum.

The policy gradient approach updates the parameters $\theta$ with respect to the policy performance $\rho$ by a step size $\alpha$.

$$
\Delta \theta \approx \alpha \frac{\partial \rho}{\partial \theta}
$$

** Policy Gradient Theorem

With function approximation, there are two ways of formulating the agent's objective. The average-reward formulation, in which policies are ranked according to their long term expected reward per step, and the start-state formulation, in which the policies are ranked according to the expected long term reward of a given start state.

The performance of a policy and the value of a state-action pair with an average-reward formulation is:

$$
\rho(\pi) = \lim_{n\to\infty} \frac{1}{n} E(r_1 + r_2 + ... + r_n)
$$

$$
Q^\pi(s,a) = \sum_{t=1}^{\infty} E(r_t - \rho(\pi))
$$

The performance of a policy and the value of a state-action pair with a start-state formulation is:

$$
\rho(\pi) = E \sum_{t=1}^{\infty} \gamma^{t-1} r_t
$$

$$
Q^\pi(s,a) = E \sum_{k=1}^{\infty} \gamma^{k-1} r_{t+k}
$$

For both of these formulations, the policy gradient for any Markov Decision Process can be defined as

$$
\frac{\partial \rho}{\partial \theta} = \sum_s d^{\pi}(s) \sum_a \frac{\partial \pi(s, a)}{\partial \theta} Q^{\pi}(s, a)
$$

** Policy Gradient with Function Approximation

The state-action value function $Q^{\pi}$ is not normally known and must be estimated. If a learned function approximator $f_w$ is sufficiently good, we can use it in place of $Q^{\pi}$ and still point roughly in the direction of the gradient.

When $f_w$ converges to a local optimum, we get:

$$
\sum_s d^{\pi}(s) \sum_a  \frac{\partial \pi(s, a)}{\partial \theta}[Q^{\pi}(s, a) - f_w(s,a)] = 0
$$

which tells us that the error in $f_w(s,a)$ is orthogonal to the gradient of the policy parameterization. Because the expression is 0 at the local optimum, we can subtract it from the policy gradient to get:


\begin{aligned}
\frac{\partial \rho}{\partial \theta} &= \sum_s d^{\pi}(s) \sum_a \frac{\partial \pi(s, a)}{\partial \theta} Q^{\pi}(s, a) - \sum_s d^{\pi}(s) \sum_a  \frac{\partial \pi(s, a)}{\partial \theta}[Q^{\pi}(s, a) - f_w(s,a)] \\\\
&= \sum_s d^{\pi}(s)  \sum_a  \frac{\partial \pi(s, a)}{\partial \theta}[Q^{\pi}(s, a) - Q^{\pi}(s,a) + f_w(s,a)] \\\\
&= \sum_s d^{\pi}(s)  \sum_a  \frac{\partial \pi(s, a)}{\partial \theta} f_w(s,a)
\end{aligned}




** Policy Iteration with Function Approximation

Given the previous theorems, we can prove that a form of policy iteration is convergent to a locally optimal policy.

$$
\theta_{k+1} = \theta_k + \alpha_k \sum_s d^{\pi_k}(s) \sum_a \frac{\partial \pi_k(s,a)}{\partial \theta} f_{w_k}(s,a)
$$

* Related
- [[/reinforcement-learning][Reinforcement Learning]]
