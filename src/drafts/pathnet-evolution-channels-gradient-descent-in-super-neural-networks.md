+++
title = "PathNet: Evolution Channels Gradient Descent in Super Neural Networks (2017)"
slug = "pathnet-evolution-channels-gradient-descent-in-super-neural-networks"
authors = "Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu, Alexander Pritzel, Daan Wierstra"
keywords = "Giant Networks, Path Evolution Algorithm, Evolution and Learning, Continual Learning, Transfer Learning, MultiTask Learning, Basal Ganglia"
source: "https://arxiv.org/pdf/1701.08734.pdf"
excerpt = "This paper establishes the policy gradient theorem and shows that policy gradient algorithms converge to a local optima for any arbitrary differential function..."
date = "2020-04-01"
+++

Authors: Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu, Alexander Pritzel, Daan Wierstra
Source: [PDF](https://arxiv.org/pdf/1701.08734.pdf)

# Summary

What will the neural networks of the future look like? These researchers suggest that shared training of one super giant neural network, capable of learning many varied tasks, is a step towards artificial general intelligence.

PathNet introduces an evolutionary algorithm where agents are embedded in the network to find good pathways to re-use for new tasks.

# Critique

# Discussion